{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kinsa Health Weather data scraper\n",
    "\n",
    "Kinsa is a company that makes Internet-connected thermometers and publishes the subsequent temperature readings in a \"US Health Weather Map\": https://healthweather.us/?mode=Atypical. Such readings can be classified as an illness when the user has a fever, and from there Kinsa uses an epidemiological model (https://content.kinsahealth.com/covid-detection-technical-approach) to estimate observed %-illness vs what would be expected under their model.\n",
    "\n",
    "This data has been praised as a pretty decent measure of flu and COVID-19 outbreaks, so below we present a scraper that extracts timeseries data for the \"observed\" and \"atypical\" rates of illness published on the US Health Weather Map website.\n",
    "\n",
    "\n",
    "### How to use:\n",
    "1. Please see the README.md for all package/software requirements.\n",
    "2. In code cell 2, select a state with the IPython notebook widget in code cell number 2. Selecting \"(all)\", the default, will scrape every county's timeseries data across all US states and territories.\n",
    "3. Continue running the code cells. The scraped data will be saved to a .csv file in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550e8a85707b4a699bc9eb42b87e06c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select state:', options=('(all)', 'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'DC', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- load in 5-digit FIPS codes for counties.\n",
    "fips_file = os.path.join('..', 'data', 'ZIP-COUNTY-FIPS_2019-12.csv')\n",
    "fips_df = pd.read_csv(fips_file\n",
    "                     , dtype = {'STCOUNTYFP': 'str'})\n",
    "\n",
    "# -- user to select which state from which they want to scrape Kinsa Health Weather data.\n",
    "# -- default: scrape all counties across all states (takes a long time!)\n",
    "state_widget = widgets.Dropdown(options = ['(all)'] + fips_df.STATE.unique().tolist()\n",
    "                                , value = '(all)'\n",
    "                                , description = 'Select state:'\n",
    "                                , disabled = False)\n",
    "display(state_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- kick up selenium chrome browser. This will pop open a chrome browser on your desktop.\n",
    "option = webdriver.ChromeOptions()\n",
    "#option.add_argument('--incognito')\n",
    "browser = webdriver.Chrome(executable_path='../chromedriver', options=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- colors of normal, anomalous points\n",
    "colors = {'observed': '#f98700'\n",
    "         , 'atypical': '#cb3547'}\n",
    "\n",
    "# -- URL we'll be scraping\n",
    "base_url = 'https://healthweather.us/?regionId={fips}&mode=Atypical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin scraping Health Weather data for Apache County, AZ... (1/15)\n",
      "Successfully scraped 95 Health Weather data points for Apache County, AZ.\n",
      "Begin scraping Health Weather data for Cochise County, AZ... (2/15)\n",
      "Successfully scraped 116 Health Weather data points for Cochise County, AZ.\n",
      "Begin scraping Health Weather data for Coconino County, AZ... (3/15)\n",
      "Successfully scraped 115 Health Weather data points for Coconino County, AZ.\n",
      "Begin scraping Health Weather data for Gila County, AZ... (4/15)\n",
      "Successfully scraped 95 Health Weather data points for Gila County, AZ.\n",
      "Begin scraping Health Weather data for Graham County, AZ... (5/15)\n",
      "Successfully scraped 119 Health Weather data points for Graham County, AZ.\n",
      "Begin scraping Health Weather data for Greenlee County, AZ... (6/15)\n",
      "Successfully scraped 95 Health Weather data points for Greenlee County, AZ.\n",
      "Begin scraping Health Weather data for La Paz County, AZ... (7/15)\n",
      "Successfully scraped 95 Health Weather data points for La Paz County, AZ.\n",
      "Begin scraping Health Weather data for Maricopa County, AZ... (8/15)\n",
      "Successfully scraped 114 Health Weather data points for Maricopa County, AZ.\n",
      "Begin scraping Health Weather data for Mohave County, AZ... (9/15)\n",
      "Successfully scraped 110 Health Weather data points for Mohave County, AZ.\n",
      "Begin scraping Health Weather data for Navajo County, AZ... (10/15)\n",
      "Successfully scraped 119 Health Weather data points for Navajo County, AZ.\n",
      "Begin scraping Health Weather data for Pima County, AZ... (11/15)\n",
      "Successfully scraped 112 Health Weather data points for Pima County, AZ.\n",
      "Begin scraping Health Weather data for Pinal County, AZ... (12/15)\n",
      "Successfully scraped 118 Health Weather data points for Pinal County, AZ.\n",
      "Begin scraping Health Weather data for Santa Cruz County, AZ... (13/15)\n",
      "Successfully scraped 95 Health Weather data points for Santa Cruz County, AZ.\n",
      "Begin scraping Health Weather data for Yavapai County, AZ... (14/15)\n",
      "Successfully scraped 108 Health Weather data points for Yavapai County, AZ.\n",
      "Begin scraping Health Weather data for Yuma County, AZ... (15/15)\n",
      "Successfully scraped 110 Health Weather data points for Yuma County, AZ.\n"
     ]
    }
   ],
   "source": [
    "df_list = list()\n",
    "\n",
    "# -- drill down to state of interest.\n",
    "state = state_widget.value\n",
    "if state != '(all)':\n",
    "    fips_codes = fips_df[fips_df.STATE == state]['STCOUNTYFP'].unique()\n",
    "else:\n",
    "    fips_codes = fips_df['STCOUNTYFP'].unique()\n",
    "\n",
    "n_fips = len(fips_codes)\n",
    "\n",
    "# -- scrape Health Weather data for each 5-digit state/county FIPS code \n",
    "for i in range(n_fips):\n",
    "    \n",
    "    # -- identify county, state for scrape.\n",
    "    fips = fips_codes[i]\n",
    "    county, state = fips_df[fips_df['STCOUNTYFP'] == fips][['COUNTYNAME', 'STATE']].drop_duplicates().iloc[0, :]\n",
    "    print(f'Begin scraping Health Weather data for {county}, {state}... ({i + 1}/{n_fips})')\n",
    "    \n",
    "    # -- navigate browser to this county\n",
    "    browser.get(base_url.format(fips=fips))\n",
    "\n",
    "    # -- wait to scrape site until %-illness data has been rendered.\n",
    "    try:\n",
    "        WebDriverWait(browser, 60).until(EC.visibility_of_element_located((By.XPATH, f\"//*[name()='svg']//*[name()='circle' and @fill='#f98700']\")))\n",
    "    except TimeoutException:\n",
    "        print(f'Timed out waiting for {county} Healthweather to load')\n",
    "        browser.quit()\n",
    "\n",
    "    # -- figure out the scale of the y-axis in terms of %-illness and pixels\n",
    "    y_scale = dict()\n",
    "\n",
    "    y_tix = browser.find_elements_by_xpath(\"//*[name()='g']//*[name()='text' and @class='recharts-text recharts-cartesian-axis-tick-value']\")\n",
    "    for y_tick in y_tix:\n",
    "        y_scale[int(y_tick.text)] = y_tick.get_property('y')['baseVal'][0]['value']\n",
    "\n",
    "    # -- SVG y-scale starts from top-left corner, not bottom left (as in Cartesian coordinates).\n",
    "    # -- Therefore 0%-illness = max(y).\n",
    "    y_max = y_scale.get(0)\n",
    "\n",
    "    # -- scale pixels to %-illness.\n",
    "    max_illness = max(y_scale.keys())\n",
    "    pixel_to_illness = max_illness / (y_scale[0] - y_scale[max_illness])\n",
    "\n",
    "    x_start = browser.find_elements_by_xpath(\"//*[name()='g']//*[name()='text' and @class='recharts-text' and @text-anchor='start']\")\n",
    "    start_date = x_start[0].text\n",
    "\n",
    "    # -- store (x, y) representing dates, illness rates in the atypical vs. expected graph.\n",
    "    data_dict = {'observed': {'x': list()\n",
    "                             , 'y': list()}\n",
    "                , 'atypical': {'x': list()\n",
    "                              , 'y': list()}}\n",
    "\n",
    "    # -- scrape observed, atypical %-illness points.\n",
    "    for color in colors:\n",
    "        circles = browser.find_elements_by_xpath(f\"//*[name()='svg']//*[name()='circle' and @fill='{colors.get(color)}']\")\n",
    "        for circle in circles:\n",
    "            x, y = circle.value_of_css_property('cx'), circle.value_of_css_property('cy')\n",
    "            x = float(x.replace('px', ''))\n",
    "            y = float(y.replace('px', ''))\n",
    "            data_dict[color]['x'].append(x)\n",
    "            data_dict[color]['y'].append(pixel_to_illness * (y_max - y))\n",
    "\n",
    "    # -- store data into DataFrames... start with observed %-illness\n",
    "    df_observed = pd.DataFrame({'x': data_dict['observed']['x']\n",
    "                               , 'y': data_dict['observed']['y']\n",
    "                               , 'condition': 'observed'})\n",
    "    df_observed.drop_duplicates('x'\n",
    "                                , inplace=True)\n",
    "    df_observed['date'] = [datetime.strptime(start_date + ' 2020', '%b %d %Y') + timedelta(days = x + 1)\n",
    "                           for x in range(df_observed.shape[0])]\n",
    "    \n",
    "    # -- check that we actually scraped data...\n",
    "    if df_observed.empty:\n",
    "        raise ValueError(f'observed %-illness data for {county}, {state} is empty!')\n",
    "   \n",
    "    # -- store %-atypical illness into DataFrame\n",
    "    df_atypical = pd.DataFrame({'x': data_dict['atypical']['x']\n",
    "                               , 'y': data_dict['atypical']['y']\n",
    "                               , 'condition': 'atypical'})\n",
    "    df_atypical.drop_duplicates('x'\n",
    "                               , inplace=True)\n",
    "    \n",
    "    # -- since atypical illness data is a subset of observed illness data,\n",
    "    # -- we determine dates of atypical illness by joining the two datasets on the x-axis pixel location.\n",
    "    df_atypical = pd.merge(df_observed[['x', 'date']]\n",
    "                           , right = df_atypical\n",
    "                           , on = 'x')\n",
    "        \n",
    "    # -- stack observed, atypical datasets.\n",
    "    df = pd.concat([df_observed, df_atypical])\n",
    "    \n",
    "    # -- append geoloco metadata.\n",
    "    df['county'] = county\n",
    "    df['state'] = state\n",
    "    df['fips'] = fips\n",
    "    print(f'Successfully scraped {df.shape[0]} Health Weather data points for {county}, {state}.')\n",
    "    \n",
    "    df_list.append(df)\n",
    "    \n",
    "# -- close browser.\n",
    "browser.close()\n",
    "\n",
    "# -- concatenate county-level DataFrames.\n",
    "df = pd.concat(df_list)\n",
    "df['illness'] = df['y']\n",
    "df.drop(['x', 'y']\n",
    "        , axis=1\n",
    "        , inplace=True)\n",
    "\n",
    "# -- save scraped data to disk.\n",
    "df.to_csv(os.path.join('..', 'data', f'{state}_health_weather.csv')\n",
    "          , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_county_health_weather(X, county, state, figsize=[12, 9]):\n",
    "    \"\"\"\n",
    "    Plot a county's Kinsa Health Weather timeseries data.\n",
    "    \n",
    "    Args\n",
    "    ---------\n",
    "    X: {pandas DataFrame} Kinsa Health Weather dataframe. Must have 'county',\n",
    "        'condition', 'date', and 'illness' columns.\n",
    "    county: {str} name of a county in the 'county' field in X\n",
    "    state: {str} name of a state in the 'state' field in X\n",
    "    figsize:\n",
    "    \"\"\"\n",
    "    if list(set(['county', 'condition', 'date', 'illness']) - set(X.columns)):\n",
    "        raise ValueError('X is missing either a \"county\", \"condition\", \"date\", or \"illness\" column.')\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize = figsize)\n",
    "    for (condition, lty) in (('observed', 'b-'), ('atypical', 'ro')):\n",
    "        plot_df = df[((df.county == county) & (df.state == state) & (df.condition == condition))]\n",
    "\n",
    "        if not plot_df.empty:\n",
    "            ax.plot(plot_df.date\n",
    "                    , plot_df.illness\n",
    "                    , lty\n",
    "                    , label = condition)\n",
    "            \n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "\n",
    "    ax.set_title(f'{county}, {state} %-illness')\n",
    "    ax.set_ylabel('%-illness')\n",
    "    ax.legend(loc = 'upper right')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_cc = plot_county_health_weather(df\n",
    "                                  , county = 'Cook County'\n",
    "                                  , state = 'IL')\n",
    "\n",
    "p_cc.figure.savefig('cook_county_il.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_lc = plot_county_health_weather(df\n",
    "                                  , county = 'Lake County'\n",
    "                                  , state = 'IL')\n",
    "\n",
    "p_lc.figure.savefig('lake_county_il.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_county_health_weather(df\n",
    "                           , county = 'DuPage County'\n",
    "                           , state='IL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
